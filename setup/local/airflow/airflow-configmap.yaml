#
# kubectl -n apache-airflow apply -f setup/local/airflow/airflow-configmap.yaml
#
# kubectl -n apache-airflow delete -f setup/local/airflow/airflow-configmap.yaml
#
#
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-configmap
data:
  airflow.cfg: |
    [celery]
    worker_concurrency = 8
    
    [celery_kubernetes_executor]
    kubernetes_queue = kubernetes
    
    [core]
    colored_console_log = False
    dags_folder = /opt/airflow/dags
    executor = KubernetesExecutor
    load_examples = False
    remote_logging = False
    plugins_folder = /opt/airflow/plugins
    
    [database]
    sql_alchemy_conn = postgresql+psycopg2://pgadmin:pgadmin@postgres.postgres.svc.cluster.local:5432/airflow
    
    [elasticsearch]
    json_format = True
    log_id_template = {dag_id}_{task_id}_{execution_date}_{try_number}
    
    [elasticsearch_configs]
    max_retries = 3
    retry_timeout = True
    timeout = 30
    
    [kerberos]
    ccache = /var/kerberos-ccache/cache
    keytab = /etc/airflow.keytab
    principal = airflow@example.com
    reinit_frequency = 3600
    
    [kubernetes_executor]
    airflow_configmap = airflow-configmap
    airflow_local_settings_configmap = airflow-configmap
    multi_namespace_mode = False
    namespace = apache-airflow
    pod_template_file = /opt/airflow/templates/volume_dag_pod_tpl.yaml
    worker_container_repository = apache/airflow
    worker_container_tag = 2.5.1
    worker_container_image_pull_policy = IfNotPresent
    worker_service_account_name = airflow
    delete_worker_pods = True
    dags_in_image = False
    dags_volume_claim = airflow-dags
    logs_volume_claim = airflow-logs
    in_cluster = True
    
    #
    git_repo = https://github.com/{{CONFIGMAP_GIT_REPO}}.git
    git_branch = {{CONFIGMAP_BRANCH}}
    git_subpath = airflow/contrib/example_dags/
    git_user = brijeshdhaker
    git_password = Accoo7@k47
    git_sync_root = /git
    git_sync_path = repo
    git_dags_folder_mount_point = {{CONFIGMAP_GIT_DAGS_FOLDER_MOUNT_POINT}}
    
    [kubernetes_node_selectors]
    # The Key-value pairs to be given to worker pods.
    # The worker pods will be scheduled to the nodes of the specified key-value pairs.
    # Should be supplied in the format: key = value
    
    [kubernetes_annotations]
    # The Key-value annotations pairs to be given to worker pods.
    # Should be supplied in the format: key = value
    
    [kubernetes_secrets]
    SQL_ALCHEMY_CONN = airflow-secrets=sql_alchemy_conn
    
    [logging]
    colored_console_log = False
    remote_logging = False
    base_log_folder = /opt/airflow/logs
    
    [metrics]
    statsd_host = apache-ariflow-statsd
    statsd_on = True
    statsd_port = 9125
    statsd_prefix = airflow
    
    [scheduler]
    run_duration = 41460
    statsd_host = apache-ariflow-statsd
    statsd_on = True
    statsd_port = 9125
    statsd_prefix = airflow
    
    [webserver]
    base_url = http://0.0.0.0:8080
    web_server_host = 0.0.0.0
    web_server_port = 8080
    enable_proxy_fix = True
    rbac = True

  airflow_local_settings.py: |
    
    
    from airflow.www.utils import UIAlert
    
    DASHBOARD_UIALERTS = [
      UIAlert(
        'Usage of a dynamic webserver secret key detected. We recommend a static webserver secret key instead.'
        ' See the <a href='
        '"https://airflow.apache.org/docs/helm-chart/stable/production-guide.html#webserver-secret-key">'
        'Helm Chart Production Guide</a> for more details.',
        category="warning",
        roles=["Admin"],
        html=True,
      )
    ]
