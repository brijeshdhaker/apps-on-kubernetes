# kubectl -n k8s-spark create cm spark-apps-configmap  \
# --from-file=spark-defaults.conf=setup/history-server/spark-defaults.conf \
# --from-file=spark.logserver.conf=setup/history-server/spark.logserver.conf \
# --from-literal=workload=HistoryServer \
# --dry-run=client -o yaml

#
# kubectl -n spark-apps apply -f setup/spark-apps/spark-app-configmap.yaml
#
# kubectl -n spark-apps delete -f setup/spark-apps/spark-app-configmap.yaml
#
---
apiVersion: v1
data:
  spark-defaults.conf: |-
    # Example:
    spark.serializer                          org.apache.spark.serializer.KryoSerializer

    spark.eventLog.enabled				      true
    spark.eventLog.dir				          file:///mnt/nfs-share/spark-logs/
    spark.eventLog.rolling.enabled            true
    spark.eventLog.rolling.maxFileSize        128m
    
    #
  spark.logserver.conf: |-
    #
    # History Server Related Properties
    #
    spark.history.provider                    org.apache.spark.deploy.history.FsHistoryProvider
    spark.history.fs.logDirectory             file:///mnt/nfs-share/spark-logs/
    spark.history.fs.update.interval          10s
    spark.history.ui.port                     18080
    spark.yarn.historyServer.allowTracking    true
    
    #
    #
  influxdb.metrics.properties: |-
    # Example configuration for Graphite sink
    *.sink.graphite.class=org.apache.spark.metrics.sink.GraphiteSink
    *.sink.graphite.host=influxdb.influxdb.svc.cluster.local
    *.sink.graphite.port=2003
    *.sink.graphite.period=10
    *.sink.graphite.unit=seconds
    *.sink.graphite.prefix=graphite
    
    # Enable JvmSource for instance master, worker, driver and executor
    master.source.jvm.class=org.apache.spark.metrics.source.JvmSource
    worker.source.jvm.class=org.apache.spark.metrics.source.JvmSource
    driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource
    executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource
    
    #
    #
  graphite.metrics.properties: |-
    # Example configuration for Graphite sink
    *.sink.graphite.class=org.apache.spark.metrics.sink.GraphiteSink
    *.sink.graphite.host=graphite-exporter.graphite.svc.cluster.local
    *.sink.graphite.port=9109
    *.sink.graphite.period=10
    *.sink.graphite.unit=seconds
    *.sink.graphite.protocol=udp
    *.sink.graphite.prefix=graphite
    
    # Enable JvmSource for instance master, worker, driver and executor
    *.source.jvm.class=org.apache.spark.metrics.source.JvmSource

    #
    #
  prometheus.metrics.properties: |-
    #
    # Example configuration for PrometheusServlet
    #
    *.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet
    *.sink.prometheusServlet.path=/metrics/prometheus
    master.sink.prometheusServlet.path=/metrics/master/prometheus
    applications.sink.prometheusServlet.path=/metrics/applications/prometheus
    
    # Enable JvmSource for instance master, worker, driver and executor
    master.source.jvm.class=org.apache.spark.metrics.source.JvmSource
    worker.source.jvm.class=org.apache.spark.metrics.source.JvmSource
    driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource
    executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource
    
    #
    #
    
    #
    #
  jmx-spark-3-0.yml: |-
    rules:
    
      # These come from the master
      # Example: master.aliveWorkers
      - pattern: "metrics<name=master\\.(.*), type=counters><>Value"
        name: spark_master_$1
    
      # These come from the worker
      # Example: worker.coresFree
      - pattern: "metrics<name=worker\\.(.*), type=counters><>Value"
        name: spark_worker_$1
    
      # These come from the application driver
      # Example: app-20160809000059-0000.driver.DAGScheduler.stage.failedStages
      - pattern: "metrics<name=(.*)\\.driver\\.(DAGScheduler|BlockManager|jvm)\\.(.*), type=gauges><>Value"
        name: spark_driver_$2_$3
        type: GAUGE
        labels:
          app_id: "$1"
    
      # These come from the application driver
      # Emulate timers for DAGScheduler like messagePRocessingTime
      - pattern: "metrics<name=(.*)\\.driver\\.DAGScheduler\\.(.*), type=counters><>Count"
        name: spark_driver_DAGScheduler_$2_total
        type: COUNTER
        labels:
          app_id: "$1"
    
      - pattern: "metrics<name=(.*)\\.driver\\.HiveExternalCatalog\\.(.*), type=counters><>Count"
        name: spark_driver_HiveExternalCatalog_$2_total
        type: COUNTER
        labels:
          app_id: "$1"
    
      # These come from the application driver
      # Emulate histograms for CodeGenerator
      - pattern: "metrics<name=(.*)\\.driver\\.CodeGenerator\\.(.*), type=counters><>Count"
        name: spark_driver_CodeGenerator_$2_total
        type: COUNTER
        labels:
          app_id: "$1"
    
      # These come from the application driver
      # Emulate timer (keep only count attribute) plus counters for LiveListenerBus
      - pattern: "metrics<name=(.*)\\.driver\\.LiveListenerBus\\.(.*), type=counters><>Count"
        name: spark_driver_LiveListenerBus_$2_total
        type: COUNTER
        labels:
          app_id: "$1"
    
      # Get Gauge type metrics for LiveListenerBus
      - pattern: "metrics<name=(.*)\\.driver\\.LiveListenerBus\\.(.*), type=gauges><>Value"
        name: spark_driver_LiveListenerBus_$2
        type: GAUGE
        labels:
          app_id: "$1"
    
      # These come from the application driver if it's a streaming application
      # Example: app-20160809000059-0000.driver.com.example.ClassName.StreamingMetrics.streaming.lastCompletedBatch_schedulingDelay
      - pattern: "metrics<name=(.*)\\.driver\\.(.*)\\.StreamingMetrics\\.streaming\\.(.*), type=gauges><>Value"
        name: spark_driver_streaming_$3
        labels:
          app_id: "$1"
          app_name: "$2"
    
      # These come from the application driver if it's a structured streaming application
      # Example: app-20160809000059-0000.driver.spark.streaming.QueryName.inputRate-total
      - pattern: "metrics<name=(.*)\\.driver\\.spark\\.streaming\\.(.*)\\.(.*), type=gauges><>Value"
        name: spark_driver_structured_streaming_$3
        labels:
          app_id: "$1"
          query_name: "$2"
    
      # These come from the application executors
      # Examples:
      #  app-20160809000059-0000.0.executor.threadpool.activeTasks (value)
      #  app-20160809000059-0000.0.executor.JvmGCtime (counter)
    
      # filesystem metrics are declared as gauge metrics, but are actually counters
      - pattern: "metrics<name=(.*)\\.(.*)\\.executor\\.filesystem\\.(.*), type=gauges><>Value"
        name: spark_executor_filesystem_$3_total
        type: COUNTER
        labels:
          app_id: "$1"
          executor_id: "$2"
    
      - pattern: "metrics<name=(.*)\\.(.*)\\.executor\\.(.*), type=gauges><>Value"
        name: spark_executor_$3
        type: GAUGE
        labels:
          app_id: "$1"
          executor_id: "$2"
    
      - pattern: "metrics<name=(.*)\\.(.*)\\.executor\\.(.*), type=counters><>Count"
        name: spark_executor_$3_total
        type: COUNTER
        labels:
          app_id: "$1"
          executor_id: "$2"
    
      - pattern: "metrics<name=(.*)\\.(.*)\\.ExecutorMetrics\\.(.*), type=gauges><>Value"
        name: spark_executor_$3
        type: GAUGE
        labels:
          app_id: "$1"
          executor_id: "$2"
    
      # These come from the application executors
      # Example: app-20160809000059-0000.0.jvm.threadpool.activeTasks
      - pattern: "metrics<name=(.*)\\.([0-9]+)\\.(jvm|NettyBlockTransfer)\\.(.*), type=gauges><>Value"
        name: spark_executor_$3_$4
        type: GAUGE
        labels:
          app_id: "$1"
          executor_id: "$2"
    
      - pattern: "metrics<name=(.*)\\.([0-9]+)\\.HiveExternalCatalog\\.(.*), type=counters><>Count"
        name: spark_executor_HiveExternalCatalog_$3_total
        type: COUNTER
        labels:
          app_id: "$1"
          executor_id: "$2"
    
      # These come from the application driver
      # Emulate histograms for CodeGenerator
      - pattern: "metrics<name=(.*)\\.([0-9]+)\\.CodeGenerator\\.(.*), type=counters><>Count"
        name: spark_executor_CodeGenerator_$3_total
        type: COUNTER
        labels:
          app_id: "$1"
          executor_id: "$2"

    #
    #
  jmx-spark-2-0.yml: |-
    rules:
    
      # These come from the master
      # Example: master.aliveWorkers
      - pattern: "metrics<name=master\\.(.*)><>Value"
        name: spark_master_$1
    
      # These come from the worker
      # Example: worker.coresFree
      - pattern: "metrics<name=worker\\.(.*)><>Value"
        name: spark_worker_$1
    
      # These come from the application driver
      # Example: app-20160809000059-0000.driver.DAGScheduler.stage.failedStages
      - pattern: "metrics<name=(.*)\\.driver\\.(DAGScheduler|BlockManager|jvm)\\.(.*)><>Value"
        name: spark_driver_$2_$3
        type: GAUGE
        labels:
          app_id: "$1"
    
      # These come from the application driver
      # Emulate timers for DAGScheduler like messagePRocessingTime
      - pattern: "metrics<name=(.*)\\.driver\\.DAGScheduler\\.(.*)><>Count"
        name: spark_driver_DAGScheduler_$2_total
        type: COUNTER
        labels:
          app_id: "$1"
    
      - pattern: "metrics<name=(.*)\\.driver\\.HiveExternalCatalog\\.(.*)><>Count"
        name: spark_driver_HiveExternalCatalog_$2_total
        type: COUNTER
        labels:
          app_id: "$1"
    
      # These come from the application driver
      # Emulate histograms for CodeGenerator
      - pattern: "metrics<name=(.*)\\.driver\\.CodeGenerator\\.(.*)><>Count"
        name: spark_driver_CodeGenerator_$2_total
        type: COUNTER
        labels:
          app_id: "$1"
    
      # These come from the application driver
      # Emulate timer (keep only count attribute) plus counters for LiveListenerBus
      - pattern: "metrics<name=(.*)\\.driver\\.LiveListenerBus\\.(.*)><>Count"
        name: spark_driver_LiveListenerBus_$2_total
        type: COUNTER
        labels:
          app_id: "$1"
    
      # Get Gauge type metrics for LiveListenerBus
      - pattern: "metrics<name=(.*)\\.driver\\.LiveListenerBus\\.(.*)><>Value"
        name: spark_driver_LiveListenerBus_$2
        type: GAUGE
        labels:
          app_id: "$1"
    
      # These come from the application driver if it's a streaming application
      # Example: app-20160809000059-0000.driver.com.example.ClassName.StreamingMetrics.streaming.lastCompletedBatch_schedulingDelay
      - pattern: "metrics<name=(.*)\\.driver\\.(.*)\\.StreamingMetrics\\.streaming\\.(.*)><>Value"
        name: spark_driver_streaming_$3
        labels:
          app_id: "$1"
          app_name: "$2"
    
      # These come from the application driver if it's a structured streaming application
      # Example: app-20160809000059-0000.driver.spark.streaming.QueryName.inputRate-total
      - pattern: "metrics<name=(.*)\\.driver\\.spark\\.streaming\\.(.*)\\.(.*)><>Value"
        name: spark_driver_structured_streaming_$3
        labels:
          app_id: "$1"
          query_name: "$2"
    
      # These come from the application executors
      # Examples:
      #  app-20160809000059-0000.0.executor.threadpool.activeTasks (value)
      #  app-20160809000059-0000.0.executor.JvmGCtime (counter)
    
      # filesystem metrics are declared as gauge metrics, but are actually counters
      - pattern: "metrics<name=(.*)\\.(.*)\\.executor\\.filesystem\\.(.*)><>Value"
        name: spark_executor_filesystem_$3_total
        type: COUNTER
        labels:
          app_id: "$1"
          executor_id: "$2"
    
      - pattern: "metrics<name=(.*)\\.(.*)\\.executor\\.(.*)><>Value"
        name: spark_executor_$3
        type: GAUGE
        labels:
          app_id: "$1"
          executor_id: "$2"
    
      - pattern: "metrics<name=(.*)\\.(.*)\\.executor\\.(.*)><>Count"
        name: spark_executor_$3_total
        type: COUNTER
        labels:
          app_id: "$1"
          executor_id: "$2"
    
      # These come from the application executors
      # Example: app-20160809000059-0000.0.jvm.threadpool.activeTasks
      - pattern: "metrics<name=(.*)\\.([0-9]+)\\.(jvm|NettyBlockTransfer)\\.(.*)><>Value"
        name: spark_executor_$3_$4
        type: GAUGE
        labels:
          app_id: "$1"
          executor_id: "$2"
    
      - pattern: "metrics<name=(.*)\\.([0-9]+)\\.HiveExternalCatalog\\.(.*)><>Count"
        name: spark_executor_HiveExternalCatalog_$3_total
        type: COUNTER
        labels:
          app_id: "$1"
          executor_id: "$2"
    
      # These come from the application driver
      # Emulate histograms for CodeGenerator
      - pattern: "metrics<name=(.*)\\.([0-9]+)\\.CodeGenerator\\.(.*)><>Count"
        name: spark_executor_CodeGenerator_$3_total
        type: COUNTER
        labels:
          app_id: "$1"
          executor_id: "$2"
    
    #
    #
    #
kind: ConfigMap
metadata:
  creationTimestamp: "2022-12-26T20:43:44Z"
  name: spark-apps-configmap
  namespace: spark-apps
